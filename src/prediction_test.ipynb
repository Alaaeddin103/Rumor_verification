{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlm_roberta_model import XlmRobertaModel\n",
    "from ke_mlm_model import KeMlmModel\n",
    "from data_loading import DataLoader_Data\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/data/data_finte_tune/rumor_evidence_label_pairs_test.json\"\n",
    "\n",
    "xlm_roberta_fintetund_path =\"/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/src/traind_Models/fine-tuned-xlm-roberta\"\n",
    "xlm_roberta_zero_shot_path =\"xlm-roberta-base\"\n",
    "ke_mlm_zero_shot_biden_path = \"kornosk/bert-election2020-twitter-stance-biden-KE-MLM\"\n",
    "ke_mlm_zero_shot_trump_path = \"kornosk/bert-election2020-twitter-stance-trump-KE-MLM\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xlm roberta fintuned Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6789\n",
      "Precision: 0.6719\n",
      "Recall: 0.6789\n",
      "F1-Score: 0.6753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load dataset \n",
    "dataset = DataLoader_Data(file_path)\n",
    "data = dataset.load_data()\n",
    "\n",
    "#load model \n",
    "model = XlmRobertaModel(model_path = xlm_roberta_fintetund_path)\n",
    "\n",
    "# Store predictions and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping_reverse = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "\n",
    "# Make predictions for each entry in the dataset\n",
    "for entry in data:\n",
    "    rumor_text = entry['rumor']\n",
    "    evidence_text = entry['evidence']\n",
    "    true_label = entry['label']\n",
    "    \n",
    "    # Convert true label to numeric form\n",
    "    true_label_id = label_mapping_reverse[true_label]\n",
    "    \n",
    "    # Predict stance and get confidence\n",
    "    predicted_label,_ = model.predict_stance(rumor_text, evidence_text)\n",
    "    \n",
    "    # Convert predicted label to numeric form\n",
    "    predicted_label_id = label_mapping_reverse[predicted_label]\n",
    "    \n",
    "    # Store the predicted label and true label as numeric values\n",
    "    predicted_labels.append(predicted_label_id)\n",
    "    true_labels.append(true_label_id)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xlm roberta zero_shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4404\n",
      "Precision: 0.1939\n",
      "Recall: 0.4404\n",
      "F1-Score: 0.2693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load dataset \n",
    "dataset = DataLoader_Data(file_path)\n",
    "data = dataset.load_data()\n",
    "\n",
    "#load model \n",
    "model = XlmRobertaModel(model_path = xlm_roberta_zero_shot_path)\n",
    "\n",
    "# Store predictions and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping_reverse = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "\n",
    "# Make predictions for each entry in the dataset\n",
    "for entry in data:\n",
    "    rumor_text = entry['rumor']\n",
    "    evidence_text = entry['evidence']\n",
    "    true_label = entry['label']\n",
    "    \n",
    "    # Convert true label to numeric form\n",
    "    true_label_id = label_mapping_reverse[true_label]\n",
    "    \n",
    "    # Predict stance and get confidence\n",
    "    predicted_label,_ = model.predict_stance(rumor_text, evidence_text)\n",
    "    \n",
    "    # Convert predicted label to numeric form\n",
    "    predicted_label_id = label_mapping_reverse[predicted_label]\n",
    "    \n",
    "    # Store the predicted label and true label as numeric values\n",
    "    predicted_labels.append(predicted_label_id)\n",
    "    true_labels.append(true_label_id)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KE_MLM Biden Zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4771\n",
      "Precision: 0.2276\n",
      "Recall: 0.4771\n",
      "F1-Score: 0.3082\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "dataset = DataLoader_Data(file_path)\n",
    "data = dataset.load_data()\n",
    "\n",
    "#load model \n",
    "model = KeMlmModel(model_path = ke_mlm_zero_shot_biden_path)\n",
    "\n",
    "# Store predictions and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping_reverse = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "\n",
    "# Make predictions for each entry in the dataset\n",
    "for entry in data:\n",
    "    rumor_text = entry['rumor']\n",
    "    evidence_text = entry['evidence']\n",
    "    true_label = entry['label']\n",
    "    \n",
    "    # Convert true label to numeric form\n",
    "    true_label_id = label_mapping_reverse[true_label]\n",
    "    \n",
    "    # Predict stance and get confidence\n",
    "    predicted_label,_ = model.predict_stance(rumor_text, evidence_text)\n",
    "    \n",
    "    # Convert predicted label to numeric form\n",
    "    predicted_label_id = label_mapping_reverse[predicted_label]\n",
    "    \n",
    "    # Store the predicted label and true label as numeric values\n",
    "    predicted_labels.append(predicted_label_id)\n",
    "    true_labels.append(true_label_id)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KE_MLM Trump Zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4771\n",
      "Precision: 0.2276\n",
      "Recall: 0.4771\n",
      "F1-Score: 0.3082\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "dataset = DataLoader_Data(file_path)\n",
    "data = dataset.load_data()\n",
    "\n",
    "#load model \n",
    "model = KeMlmModel(model_path = ke_mlm_zero_shot_trump_path)\n",
    "\n",
    "# Store predictions and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping_reverse = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "\n",
    "# Make predictions for each entry in the dataset\n",
    "for entry in data:\n",
    "    rumor_text = entry['rumor']\n",
    "    evidence_text = entry['evidence']\n",
    "    true_label = entry['label']\n",
    "    \n",
    "    # Convert true label to numeric form\n",
    "    true_label_id = label_mapping_reverse[true_label]\n",
    "    \n",
    "    # Predict stance and get confidence\n",
    "    predicted_label,_ = model.predict_stance(rumor_text, evidence_text)\n",
    "    \n",
    "    # Convert predicted label to numeric form\n",
    "    predicted_label_id = label_mapping_reverse[predicted_label]\n",
    "    \n",
    "    # Store the predicted label and true label as numeric values\n",
    "    predicted_labels.append(predicted_label_id)\n",
    "    true_labels.append(true_label_id)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating XLM-Roberta Fine-Tuned Model:\n",
      "Accuracy: 0.6789\n",
      "Precision: 0.6719\n",
      "Recall: 0.6789\n",
      "F1-Score: 0.6753\n",
      "Evaluating XLM-Roberta Zero-Shot Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0826\n",
      "Precision: 0.0068\n",
      "Recall: 0.0826\n",
      "F1-Score: 0.0126\n",
      "Evaluating KE MLM Biden Fine-Tuned Model:\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'ke_mlm_biden_fine_tuned_model' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m     evaluate_model(ke_mlm_trump_model, data)\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[16], line 73\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m# KE MLM Biden Fine-Tuned\u001b[39;00m\n\u001b[1;32m     72\u001b[0m  \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEvaluating KE MLM Biden Fine-Tuned Model:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m  ke_mlm_biden_fine_tuned_model \u001b[39m=\u001b[39m KeMlmModel(model_path\u001b[39m=\u001b[39mke_mlm_biden_fine_tuned_model)\n\u001b[1;32m     74\u001b[0m  evaluate_model(ke_mlm_biden_fine_tuned_model, data)\n\u001b[1;32m     76\u001b[0m  \u001b[39m# KE MLM Biden Zero-Shot\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'ke_mlm_biden_fine_tuned_model' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from xlm_roberta_model import XlmRobertaModel\n",
    "from ke_mlm_model import KeMlmModel\n",
    "from data_loading import DataLoader_Data\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate the model using the provided data.\n",
    "    :param model: Model instance (XlmRobertaModel or KeMlmModel)\n",
    "    :param data: Loaded dataset\n",
    "    \"\"\"\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Define label mapping\n",
    "    label_mapping_reverse = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "\n",
    "    # Iterate over each sample in the dataset\n",
    "    for entry in data:\n",
    "        rumor_text = entry['rumor']\n",
    "        evidence_text = entry['evidence']\n",
    "        true_label = entry['label']\n",
    "\n",
    "        # Convert true label to numeric form\n",
    "        true_label_id = label_mapping_reverse[true_label]\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_label, _ = model.predict_stance(rumor_text, evidence_text)\n",
    "        \n",
    "        # Convert predicted label to numeric form\n",
    "        predicted_label_id = label_mapping_reverse[predicted_label]\n",
    "        \n",
    "        # Store the predicted and true labels\n",
    "        predicted_labels.append(predicted_label_id)\n",
    "        true_labels.append(true_label_id)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Output evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    file_path = \"/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/data/data_finte_tune/rumor_evidence_label_pairs_test.json\"\n",
    "    xlm_roberta_fine_tuned_path = \"/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/src/traind_Models/fine-tuned-xlm-roberta\"\n",
    "    xlm_roberta_zero_shot_path = \"xlm-roberta-base\"\n",
    "    ke_mlm_fine_tuned_path = \"/Users/alaaeddinalia/Desktop/bachelor_arbeit /Rumor_verification/src/traind_Models/fine-tuned-ke-mlm-biden\"\n",
    "    ke_mlm_zero_shot_biden_path = \"kornosk/bert-election2020-twitter-stance-biden-KE-MLM\"\n",
    "    ke_mlm_zero_shot_trump_path = \"kornosk/bert-election2020-twitter-stance-trump-KE-MLM\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = DataLoader_Data(file_path)\n",
    "    data = dataset.load_data()\n",
    "\n",
    "    # XLM-Roberta Fine-Tuned\n",
    "    print(\"Evaluating XLM-Roberta Fine-Tuned Model:\")\n",
    "    xlm_roberta_fine_tuned_model = XlmRobertaModel(model_path=xlm_roberta_fine_tuned_path)\n",
    "    evaluate_model(xlm_roberta_fine_tuned_model, data)\n",
    "    \n",
    "    # XLM-Roberta Zero-Shot\n",
    "    print(\"Evaluating XLM-Roberta Zero-Shot Model:\")\n",
    "    xlm_roberta_zero_shot_model = XlmRobertaModel(model_path=xlm_roberta_zero_shot_path)\n",
    "    evaluate_model(xlm_roberta_zero_shot_model, data)\n",
    "    \n",
    "   # KE MLM Biden Fine-Tuned\n",
    "    print(\"Evaluating KE MLM Biden Fine-Tuned Model:\")\n",
    "    ke_mlm_biden_fine_tuned_model = KeMlmModel(model_path=ke_mlm_biden_fine_tuned_model)\n",
    "    evaluate_model(ke_mlm_biden_fine_tuned_model, data)\n",
    "\n",
    "    # KE MLM Biden Zero-Shot\n",
    "    print(\"Evaluating KE MLM Biden Zero-Shot Model:\")\n",
    "    ke_mlm_biden_model = KeMlmModel(model_path=ke_mlm_zero_shot_biden_path)\n",
    "    evaluate_model(ke_mlm_biden_model, data)\n",
    "\n",
    "    # KE MLM Trump Zero-Shot\n",
    "    print(\"Evaluating KE MLM Trump Zero-Shot Model:\")\n",
    "    ke_mlm_trump_model = KeMlmModel(model_path=ke_mlm_zero_shot_trump_path)\n",
    "    evaluate_model(ke_mlm_trump_model, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cc93f8dff1c92270d493339c16f690d2b8430c71b183d15a508aeaf38ac14d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
