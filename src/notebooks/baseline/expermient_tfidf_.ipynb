{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import DataLoader_Data\n",
    "from language_detector import LanguageDetector\n",
    "from preprocessor import Preprocessor\n",
    "from feature_extractor_ import FeatureExtractor\n",
    "from similarity_calculator import SimilarityCalculator\n",
    "from rumor_classifier import RumorClassifier\n",
    "from evaluation import Evaluation\n",
    "from sentiment_analyzer import SentimentAnalyzer\n",
    "from ner_extractor import NERExtractor\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "file_path = '/Users/alaaeddinalia/Desktop/Bachelor_Arbeit_2/Rumor_verification/data/raw/English_train.json'\n",
    "preprocessed_file_path = '/Users/alaaeddinalia/Desktop/Bachelor_Arbeit_2/Rumor_verification/data/processed/English_train_preprocessed.json'\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data_loader = DataLoader_Data(file_path)\n",
    "language_detector = LanguageDetector()\n",
    "preprocessor = Preprocessor(\n",
    "    language=language_detector.detect_language(data_loader.data[0]['rumor']),\n",
    "    remove_urls=True,\n",
    "    remove_special_characters=True,\n",
    "    remove_stopwords=True,\n",
    "    remove_noise_words=True,\n",
    "    remove_emojis=True,\n",
    "    apply_stemming=True,\n",
    "    apply_lemmatization=True\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "preprocessed_data = []\n",
    "for item in data_loader.data:\n",
    "    item['rumor'] = preprocessor.preprocess_text(item['rumor'])\n",
    "    for i, timeline_entry in enumerate(item['timeline']):\n",
    "        item['timeline'][i][2] = preprocessor.preprocess_text(timeline_entry[2])\n",
    "    for j, evidence_entry in enumerate(item['evidence']):\n",
    "        item['evidence'][j][2] = preprocessor.preprocess_text(evidence_entry[2])\n",
    "    preprocessed_data.append(item)\n",
    "\n",
    "# Save preprocessed data\n",
    "with open(preprocessed_file_path, 'w') as f:\n",
    "    json.dump(preprocessed_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Perform Sentiment Analysis and Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer and NER extractor\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "ner_extractor = NERExtractor()\n",
    "\n",
    "# Apply sentiment analysis and NER to the preprocessed data\n",
    "for item in preprocessed_data:\n",
    "    item['sentiment'] = sentiment_analyzer.analyze_sentiment(item['rumor'])\n",
    "    item['entities'] = ner_extractor.extract_entities(item['rumor'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction using TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FeatureExtractor with the TF-IDF method\n",
    "extractor = FeatureExtractor(method='tfidf')\n",
    "\n",
    "# Combine all texts for feature extraction\n",
    "all_texts = [item['rumor'] for item in preprocessed_data] + \\\n",
    "            [timeline_entry[2] for item in preprocessed_data for timeline_entry in item['timeline']] + \\\n",
    "            [evidence_entry[2] for item in preprocessed_data for evidence_entry in item['evidence']]\n",
    "\n",
    "# Extract TF-IDF vectors\n",
    "vectors = extractor.fit_transform(all_texts)\n",
    "\n",
    "# Ensure vectors are a numpy array or sparse matrix (as TF-IDF returns a sparse matrix by default)\n",
    "if hasattr(vectors, 'toarray'):\n",
    "    vectors = vectors.toarray()  # Convert sparse matrix to a dense array (optional, based on your needs)\n",
    "\n",
    "# Check if vectors is a numpy array after conversion\n",
    "if isinstance(vectors, np.ndarray):\n",
    "    # Assign vectors to the data\n",
    "    index = 0\n",
    "    for item in preprocessed_data:\n",
    "        item['rumor_vector'] = vectors[index].tolist()  # Assign the TF-IDF vector for the rumor\n",
    "        index += 1\n",
    "        \n",
    "        # Assign vectors for timeline entries\n",
    "        for timeline_entry in item['timeline']:\n",
    "            timeline_entry.append(vectors[index].tolist())  # Add the vector to the timeline entry\n",
    "            index += 1\n",
    "        \n",
    "        # Assign vectors for evidence entries\n",
    "        for evidence_entry in item['evidence']:\n",
    "            evidence_entry.append(vectors[index].tolist())  # Add the vector to the evidence entry\n",
    "            index += 1\n",
    "else:\n",
    "    print(f\"Error: Expected numpy array, but got {type(vectors)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 1000)\n"
     ]
    }
   ],
   "source": [
    "rumor_vectors = vectors[:len(preprocessed_data)]  # Extract rumor vectors (first N entries)\n",
    "print(rumor_vectors.shape)  # Print the shape of the rumor vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities\n",
    "similarity_calculator = SimilarityCalculator()\n",
    "evidence_similarities = similarity_calculator.calculate_evidence_similarity(preprocessed_data, extractor)\n",
    "timeline_similarities = similarity_calculator.calculate_timeline_similarity(preprocessed_data)\n",
    "\n",
    "# Save similarity results\n",
    "with open('/Users/alaaeddinalia/Desktop/Bachelor_Arbeit_2/Rumor_verification/data/similarity/similarity_tfidf/English_train_evidence_similarity_results.json', 'w') as f:\n",
    "    json.dump(evidence_similarities, f, ensure_ascii=False, indent=4)\n",
    "with open('/Users/alaaeddinalia/Desktop/Bachelor_Arbeit_2/Rumor_verification/data/similarity/similarity_tfidf/English_train_timeline_similarity_results.json', 'w') as f:\n",
    "    json.dump(timeline_similarities, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Calculate averages for classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.030973366730171797,) (0.03360906624163678,) 0.02440079959398695\n"
     ]
    }
   ],
   "source": [
    "avg_total=similarity_calculator.calculate_average_similarity(evidence_similarities),\n",
    "avg_refutes=similarity_calculator.calculate_average_similarity([sim for sim in evidence_similarities if sim['label'] == \"REFUTES\"]),\n",
    "avg_supports=similarity_calculator.calculate_average_similarity([sim for sim in evidence_similarities if sim['label'] == \"SUPPORTS\"]) \n",
    "\n",
    "\n",
    " # Calculate averages for classification thresholds\n",
    " \n",
    " \n",
    "classifier = RumorClassifier(\n",
    "    avg_total=similarity_calculator.calculate_average_similarity(evidence_similarities),\n",
    "    avg_refutes=similarity_calculator.calculate_average_similarity([sim for sim in evidence_similarities if sim['label'] == \"REFUTES\"]),\n",
    "    avg_supports=similarity_calculator.calculate_average_similarity([sim for sim in evidence_similarities if sim['label'] == \"SUPPORTS\"])\n",
    ")\n",
    "\n",
    "print(avg_total, avg_refutes , avg_supports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the rumors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.classify(timeline_similarities, preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4353\n",
      "Recall: 0.4062\n",
      "F1 Score: 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaaeddinalia/Desktop/Bachelor_Arbeit_2/Rumor_verification/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with precision, recall, and F1\n",
    "ground_truth_labels = [item['label'] for item in preprocessed_data]\n",
    "evaluator = Evaluation(ground_truth_labels, predictions)\n",
    "metrics = evaluator.all_metrics()\n",
    "\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d34e587a68a8b1962d37ed7d17f5a5eb49ff3bc1782909a0fcc667ed67612d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
